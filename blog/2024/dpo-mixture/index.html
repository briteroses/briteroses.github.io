<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Some DPO napkin math | Brian R.Y. Huang</title> <meta name="author" content="Brian R.Y. Huang"> <meta name="description" content="TL;DR if we formulate prompt-conditioned output distributions from an LLM as a mixture of an aligned component + unaligned component, then a DPO tune " tilts the output towards aligned component diminishing---but not expunging unaligned component.> <meta name="keywords" content="AI safety, adversarial robustness, LLM factuality, perception, machine learning"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?19f3075a2d19613090fe9e16b564e1fe" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://briteroses.github.io/blog/2024/dpo-mixture/"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?e74e74bf055e5729d44a7d031a5ca6a5" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?96d6b3e1c3604aca8b6134c7afdd5db6"></script> <script src="/assets/js/dark_mode.js?9b17307bb950ffa2e34be0227f53558f"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="//"><span class="font-weight-bold">Brian </span>R.Y. Huang</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about</a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/works/">research &amp; projects</a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">teaching</a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-solid fa-moon"></i> <i class="fa-solid fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">Some DPO napkin math</h1> <p class="post-meta">December 26, 2024</p> <p class="post-tags"> <a href="//blog/2024"> <i class="fa-solid fa-calendar fa-sm"></i> 2024 </a>   ·   <a href="//blog/tag/dpo"> <i class="fa-solid fa-hashtag fa-sm"></i> dpo</a>   <a href="//blog/tag/preference-tuning"> <i class="fa-solid fa-hashtag fa-sm"></i> preference-tuning</a>   <a href="//blog/tag/alignment"> <i class="fa-solid fa-hashtag fa-sm"></i> alignment</a>   <a href="//blog/tag/math"> <i class="fa-solid fa-hashtag fa-sm"></i> math</a>     ·   <a href="//blog/category/research"> <i class="fa-solid fa-tag fa-sm"></i> research</a>   </p> </header> <article class="post-content"> <div id="markdown-content"> \[\DeclareMathOperator\supp{supp} \DeclareMathOperator\TPM{TPM} \DeclareMathOperator\unif{unif} \DeclareMathOperator\card{card}\] <p>As a mathy Christmas treat, I wanted to share some napkin math I did earlier this year while studying the preference tuning literature, especially the (now seminal) <a href="https://arxiv.org/abs/2305.18290" rel="external nofollow noopener" target="_blank">DPO paper</a>. For starters, DPO and its cousin techniques (like <a href="https://arxiv.org/abs/2402.01306" rel="external nofollow noopener" target="_blank">KTO</a> and others) hinge on optimizing a KL-constrained RL objective, which I reproduce below:</p> <p>\begin{equation} \label{eq:klrl} \mathcal{L}(\theta) = \max_\theta E_{y \in p_\theta(y|x)}[r(y;x)] - \beta\,\mathbb{D}<em>{KL}(p</em>\theta(y|x) || p_{\theta_{ref}}(y|x)). \end{equation}</p> <p>Here, \(\theta\) is our current model parameters optimized during the preference tune; \(\theta_{ref}\) is our (frozen) reference model parameters (usually, the SFTed model or base model at the initialization of the preference tune); \(x\) is a (given) prompt and \(y\) is its sampled response from the model; \(r\) is the reward function; and \(\beta\) is the hyperparameter controlling the weighting of the KL regularization term.</p> <p>The solution to the KL-constrained RL loss has a well-known form that upweights or downweights a sampled response relative to its reward. However, at least to me, this solution isn’t very intuitive for understanding how more overarching model behaviors are altered under a preference tune. As a motivating example, DPO tuning is often used as a line of defense for model safety: say, promoting refusals for a wide range of explicitly <a href="https://arxiv.org/abs/2402.04249" rel="external nofollow noopener" target="_blank">harmful intents</a>, or <a href="https://arxiv.org/abs/2401.01967" rel="external nofollow noopener" target="_blank">reducing toxicity</a> for all kinds of mundane prompts. If we care about obtaining better refusals or more nontoxic behaviors “across the board,” then we want to understand how entire probability masses corresponding to general output specifications (for example, all possible responses with toxic content) are shifted under DPO. These desired behavioral shifts can be further confuzzled in practice, especially since the reward is not known a priori and issues such as data-policy mismatch or likelihood displacement can produce pretty unintuitive results from a DPO tune. (<a href="https://tianjianl.github.io/blog/2024/dpo/" rel="external nofollow noopener" target="_blank">This blog post</a> provides a great overview of DPO failure modes, if you want to read further.)</p> <table> <tbody> <tr> <td>To capture a more macroscopic view of DPO, I find it useful to formulate the prompt-conditioned output distribution of our reference model $$ p_{\theta_{\text{ref}}}(y</td> <td>x) \(as a mixture of two component distributions, one "aligned" and the other "unaligned." We denote\) \mathcal{A}_{x}(y</td> <td>x) \(for the aligned distribution and\) \mathcal{U}_{x}(y</td> <td>x) \(, with mixture weight\) \alpha_{x} $$:</td> </tr> </tbody> </table> <p>\begin{equation} p_{\theta_{ref}}(y | x) = \alpha_x p_{\mathcal{A}<em>x}(y | x) + (1-\alpha_x) p</em>{\mathcal{U}_x}(y | x). \end{equation}</p> <table> <tbody> <tr> <td>For a (somewhat handwavy, sorry!) illustration based on our toxicity-reduction scenario, we might assign all fully nontoxic responses to the aligned component, all extremely toxic responses to the unaligned component, and a range of ambiguous responses to both components, with intra-component weightings depending on the level of toxicity. In other words, we can potentially describe more general specifications of model behavior under the umbrella of these mixture components. If we establish certain properties of the post-DPO output distribution $$ p_\theta(y</td> <td>x) \(through the lens of\) \mathcal{A} \(and\) \mathcal{U} $$, we gain insight into how DPO may induce—or fail to induce—a broader behavior such as toxicity reduction or safety refusals.</td> </tr> </tbody> </table> <p>First, we show that the post-DPO output distribution has a specific form—no longer a mixture per-se, but a per-sample tilting of the original mixture.</p> <p><strong>Lemma 1.</strong> Assume the preference-tuning objective function used is \eqref{klrl}. With no additional assumptions on \(\mathcal{A}\) and \(\mathcal{U}\), the output distribution of \(\theta\) can be written as:</p> \[p_\theta(y|x)=\alpha_\mathcal{A}(y;x)p_\mathcal{A}(y|x)+\alpha_\mathcal{U}(y;x)p_\mathcal{U}(y|x)\] \[\alpha_\mathcal{A}(y;x)=\frac{1}{Z(x)}\alpha(x)e^{r(y;x)/\beta}, \;\; \alpha_\mathcal{U}(y;x)=\frac{1}{Z(x)}(1-\alpha(x))e^{r(y;x)/\beta}\] <table> <tbody> <tr> <td>where $$ Z(x)=\sum_{y \in \supp p_\theta({\cdot}</td> <td>x)} p_{ref}(y</td> <td>x)e^{r(y;x)/\beta} \(is the partition function of\) p_\theta(y;x) $$.</td> </tr> </tbody> </table> <p>We can then prove a stronger result about shifts in the probability masses corresponding to each mixture component:</p> <table> <tbody> <tr> <td> <strong>Theorem 2.</strong> Write the “total probability mass” of \(\mathcal{A}\) in $$ p_\theta({\cdot}</td> <td>x) \(as\) \TPM(\mathcal{A};x) = \sum_{y \in \supp p_\theta({\cdot}</td> <td>x)} \alpha_\mathcal{A}(y;x)p_\mathcal{A}(y</td> <td>x) \((and define this respectively for\) \mathcal{U} \(). If the reward function\) r(y;x) $$ satisfies the following additional assumptions:</td> </tr> </tbody> </table> <ul> <li> <table> <tbody> <tr> <td>\(r(y;x)\) and $$ p_\mathcal{A}(y</td> <td>x) $$ are positively correlated</td> </tr> </tbody> </table> </li> <li> <table> <tbody> <tr> <td>\(r(y;x)\) and $$ p_\mathcal{U}(y</td> <td>x) $$ are negatively correlated</td> </tr> </tbody> </table> </li> <li>\(r\) is finite</li> </ul> <table> <tbody> <tr> <td>then the total probability masses of \(\mathcal{A}\) and \(\mathcal{U}\) in $$ p_\theta({\cdot}</td> <td>x) $$ satisfy:</td> </tr> </tbody> </table> \[1 &gt; \TPM(\mathcal{A};x) \geq \alpha(x), \;\; 0 &lt; \TPM(\mathcal{U};x) \leq (1-\alpha(x)).\] <p>The total probability masses of \(\mathcal{A}\) and \(\mathcal{U}\) can be viewed as a measure of the aligned (resp. unaligned) distribution’s influence on the tuned language model’s output distribution.</p> <h2 id="proofs">Proofs</h2> <p>First, the closed-form solution for the KL-constrained RL objective is well-known, but we reproduce it here. First, rewrite the objective as:</p> \[\begin{align*} \mathcal{L}(\theta) &amp;= \sum_y p_\theta(y|x)r(y;x) - \beta p_\theta(y|x) \log\left(\frac{p_\theta(y|x)}{p_{ref}(y|x)}\right) \\ &amp;= \beta \sum_y p_\theta(y|x) \Big[ \log e^{r(y;x)/\beta} - \log\left(\frac{p_\theta(y|x)}{p_{ref}(y|x)}\right) \Big] \\ &amp;= -\beta \sum_y p_\theta(y|x) \log\left(\frac{p_\theta(y|x)}{\frac{1}{Z(x)}p_{ref}(y|x)e^{r(y;x)/\beta}}\right) + \beta\log{Z(x)} \end{align*}\] <table> <tbody> <tr> <td>where $$ Z(x) = \sum_y p_{ref}(y</td> <td>x)e^{r(y;x)/\beta} \(is the partition function. Notice that\) \frac{1}{Z(x)}p_{ref}(y</td> <td>x)e^{r(y;x)/\beta} \(is a valid probability distribution, so by Gibbs' inequality, the objective\) \mathcal{L}(\theta) $$ is maximized when:</td> </tr> </tbody> </table> \[p_\theta(y|x) = \frac{1}{Z(x)}p_{ref}(y|x)e^{r(y;x)/\beta}.\] <table> <tbody> <tr> <td>Substituting the mixture formulation for $$ p_{ref}(y</td> <td>x) $$ and performing a little more algebra gives us:</td> </tr> </tbody> </table> \[p_\theta(y|x)=\alpha_\mathcal{A}(y;x)p_\mathcal{A}(y|x)+\alpha_\mathcal{U}(y;x)p_\mathcal{U}(y|x)\] \[\alpha_\mathcal{A}(y;x)=\frac{1}{Z(x)}\alpha(x)e^{r(y;x)/\beta}, \;\; \alpha_\mathcal{U}(y;x)=\frac{1}{Z(x)}(1-\alpha(x))e^{r(y;x)/\beta}\] <p>as Lemma 1 states.</p> <table> <tbody> <tr> <td>For Theorem 2, we assumed \(r(y;x)\) was positively correlated with $$ p_\mathcal{A}(y</td> <td>x) \(and negatively correlated with\) p_\mathcal{U}(y</td> <td>x) \(. Since\) Z(x) \(and\) \alpha(x) \(are constants with respect to\) y \(,\) \alpha_\mathcal{A}(y;x) \(and\) \alpha_\mathcal{U}(y;x) \(are clearly monotonic functions of\) r(y;x) \(, and as a result, the correlation properties are preserved:\) \alpha_\mathcal{A}(y;x) \(is positively correlated with\) p_\mathcal{A}(y</td> <td>x) \(, and\) \alpha_\mathcal{U}(y;x) \(is negatively correlated with\) p_\mathcal{U}(y</td> <td>x) $$.</td> </tr> </tbody> </table> <table> <tbody> <tr> <td>Notice that finiteness of \(r\) implies \(e^{r(y;x)/\beta} &gt; 0\) everywhere, so the output distribution of \(\theta\) must have nonzero probability for any output in \(\supp p_{ref}\). Therefore, $$ \supp p_\theta({\cdot}</td> <td>x) = \supp p_{ref}({\cdot}</td> <td>x) \(. Consider the expectations of\) \alpha_\mathcal{A}(y;x) \(and\) \alpha_\mathcal{U}(y;x) \(over a uniform distribution over (WLOG)\) \supp p_{\theta}({\cdot}</td> <td>x) $$, and use the correlation properties:</td> </tr> </tbody> </table> \[\begin{align*} E_{y \in \unif(\supp p_{\theta}({\cdot} | x))}[\alpha_\mathcal{A}(y;x)p_\mathcal{A}(y|x)] &amp;&gt; E_y[\alpha_\mathcal{A}(y;x)]E_y[p_\mathcal{A}(y|x)] \\ &amp;= \frac{1}{Z(x)}\alpha(x)E_y[e^{r(y;x)/\beta}]E_y[p_\mathcal{A}(y|x)]. \end{align*}\] \[\begin{align*} E_y[\alpha_\mathcal{U}(y;x)p_\mathcal{U}(y|x)] &amp;&lt; E_y[\alpha_\mathcal{U}(y;x)]E_y[p_\mathcal{U}(y|x)] \\ &amp;= \frac{1}{Z(x)}(1-\alpha(x))E_y[e^{r(y;x)/\beta}]E_y[p_\mathcal{U}(y|x)]. \end{align*}\] <table> <tbody> <tr> <td>(For the sake of brevity, we mostly omit writing the distribution over which the expectation is taken; for the rest of the proof, \(E_y[{\cdot}]\) is shorthand for $$ E_{y \in \unif(\supp p_{\theta}({\cdot}</td> <td>x))}[{\cdot}] \(.) The\) E_y[e^{r(y;x)/\beta}] $$ and partition function terms vanish upon dividing the inequalities:</td> </tr> </tbody> </table> \[\frac{E_y[\alpha_\mathcal{A}(y;x)p_\mathcal{A}(y|x)]}{E_y[\alpha_\mathcal{U}(y;x)p_\mathcal{U}(y|x)]} &gt; \frac{E_y[\alpha(x)p_\mathcal{A}(y|x)]}{E_y[(1-\alpha(x))p_\mathcal{U}(y|x)]}\] <p>Reciprocating the inequality, adding 1 to both sides, and reciprocating again yields:</p> \[\frac{E_y[\alpha_\mathcal{A}(y;x)p_\mathcal{A}(y|x)]}{E_y[p_\theta(y|x)]} &gt; \frac{E_y[\alpha(x)p_\mathcal{A}(y|x)]}{E_y[p_{ref}(y|x)]}\] <table> <tbody> <tr> <td>Finally, we multiply all expectations by $$ \card(\supp p_\theta({\cdot}</td> <td>x)) $$. Using the property that</td> </tr> </tbody> </table> \[\card(\supp p) {\cdot} E_{y \in \unif(\supp p)}[p(x)] = 1\] <table> <tbody> <tr> <td>for any discrete probability distribution \(p\), along with our earlier observation that $$ \supp p_\theta({\cdot}</td> <td>x) = \supp p_{ref}({\cdot}</td> <td>x) \(, we see that the expectations of\) p_\theta(y</td> <td>x) \(and\) p_{ref}(y</td> <td>x) \(vanish. On the other hand, multiplying the expectation of\) \alpha_\mathcal{A}(y;x)p_\mathcal{A}(y</td> <td>x) \(by\) \card(\supp p_\theta({\cdot}</td> <td>x)) $$ gives the total probability mass. We recover:</td> </tr> </tbody> </table> \[\TPM(\mathcal{A};x) &gt; \alpha(x) \;\;\;\;\;\;\;\; \text{and likewise, } \TPM(\mathcal{U};x) &lt; (1-\alpha(x)).\] <h2 id="takeaways">Takeaways</h2> <p>Intuitively, we can interpret Theorem 2 as the following idea: if we select aligned and unaligned distributions that indeed correspond to a level of “alignment” or utility measured by our rewards, then the preference-tuned model draws outputs more heavily from the aligned distribution, while the influence of the unaligned distribution is diminished but not expunged. Crucially, the preference-tuned model still has nonzero probability of producing any harmful or otherwise undesirable output that was possible in the base model.</p> <p>Looking at our math, we do need to be careful about our construction of aligned and unaligned distributions: the reward correlation conditions in Theorem 2 are fairly reasonable, but can certainly be violated in practice. Fortunately, the reward finiteness condition should always hold; in DPO, finiteness is clear from the form of the implicit reward used.</p> <p>There’s a link here between DPO-based safety finetunes and adversarial attacks (jailbreaks) which may be fairly obvious, but that I still want to highlight. Because we can never expunge harmful generations, we can re-amplify the probability of obtaining harmful trajectories from our post-DPO model with various attack methods, from high-temperature decoding to discrete-optimized suffixes to randomized prompt fuzzing with a high attack budget. Additionally, our analysis only extends to prompt-conditioned output distributions, and we can’t say anything definitive about model behavior under prompts out-of-distribution of those in our DPO data. Under the <a href="">mismatched generalization</a> hypothesis for jailbreaks, safety finetunes lose effectiveness on out-of-distribution prompts; this proposition illuminates what may be happening in the blind spots of our analysis.</p> </div> </article> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2024 Brian R.Y. Huang. Last updated: December 26, 2024. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?7b30caa5023af4af8408a472dc4e1ebb"></script> <script defer src="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?d633890033921b33e0ceb13d22340a9c"></script> <script defer src="/assets/js/common.js?acdb9690d7641b2f8d40529018c71a01"></script> <script defer src="/assets/js/copy_code.js?07b8786bab9b4abe90d10e61f7d12ff7" type="text/javascript"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>