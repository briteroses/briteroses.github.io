---
layout: page
permalink: /notepad/
title: notepad
description: Some questions (some measured, some sketched) that I've been interested in lately.
nav: true
nav_order: 1
---

- How can we attribute potentially dangerous or harmful outputs in language model to specific input factors--say, a piece of training data, or an ingredient of the instruction tuning or preference tuning recipe? Can a better understanding of this attribution lead to a systematic understanding of model behavior in out-of-distribution or adversarial settings--and, in turn, stronger defenses against adversarial inputs?
- People have been investigating factuality in language models as a countermeasure for the "hallucination" problem. How do we reconcile the goal of factuality with confounding factors, like uncertainty and bias, inherent in human communication?
- Are current and near-future frontier models "only as good as their data"? How much mileage can we get towards grokking new model capabilities or excising undesired capabilities solely by manipulating training data and inputs?
- Weight interpolation (or in the current marketing, "model merging") is black magic that just works. Can we make it work even better?
- Also, starting to think about superalignment!