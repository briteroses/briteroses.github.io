---
layout: page
permalink: /notepad/
title: notepad
description: Some questions (some measured, some sketched) that I've been interested in lately.
nav: true
nav_order: 1
---

- How can we attribute potentially dangerous or harmful outputs in language model to specific input factors--say, a piece of training data, or an ingredient of the instruction tuning or preference tuning recipe? Can a better understanding of this attribution lead to a systematic understanding of model behavior in out-of-distribution or adversarial settings--and, in turn, stronger defenses against adversarial inputs?
- People have been investigating factuality in language models as a countermeasure for the "hallucination" problem. How do we reconcile the goal of factuality with confounding factors, like uncertainty and bias, inherent in human communication?
- Are current and near-future frontier models "only as good as their data"? How much mileage can we get towards grokking new model capabilities or excising undesired capabilities solely by manipulating training data and inputs?
- How can frontier model systems understand what is or isn't dangerous, in the face of constantly-updating online communication, coded/obfuscated language, and cultural contexts? Can a LM-based content moderation API be expediently updated with social media slang as it emerges, especially slang with the capability to bully or harm? Can this API detect and understand "dog whistles" for fascist or radically right-wing ideologies?