---
layout: about
title: about
permalink: /
subtitle: 

profile:
  align: left
  image: prof_pic.jpg
  image_circular: true # crops the image to make it circular
  more_info: >

news: false  # includes a list of news items
latest_posts: false  # includes a list of the newest posts
selected_papers: true # includes a list of papers marked as "selected={true}"
social: true  # includes social icons at the bottom of the page
---

I'm a recent graduate from MIT with Bachelor's degrees in Math & CS as well as a Master's in CS. I'm broadly interested in methods and evaluations that may engender more helpful, equitable, and harm-reducing outcomes for all stakeholders of frontier AI systems.

At MIT, I was fortunate to conduct research on adversarial robustness of deep learning models, advised by [Hadi Salman](https://hadisalman.com/) and [Aleksander MÄ…dry](https://madry.mit.edu/). I was also a teaching assistant for MIT's flagship graduate-level machine learning class (6.867, now [6.7900](https://gradml.mit.edu/)) and for the statistical data analysis class (6.3720/3722). Previously, in the year before entering MIT, I researched black hole formation in general relativity with [Marcus Khuri](https://www.math.stonybrook.edu/cards/khurimarcus.html) at Stony Brook University. I've also worked on causal mediation analysis for mechanistic interpretability as a research intern at [Redwood Research](https://www.redwoodresearch.org/). I'm currently working on interesting perception problems as a research engineer at [Matic](https://maticrobots.com/); post-graduation, I've continued my involvement in various research collaborations, studying the mechanisms behind factuality, adversarial prompting, and other phenomena in frontier language models.

Some questions (some measured, some sketched) that I've been interested in lately:
- How can we attribute potentially dangerous or harmful outputs in language model to specific input factors--say, a piece of training data, or an ingredient of the instruction tuning or preference tuning recipe? Can a better understanding of this attribution lead to a systematic understanding of model behavior in out-of-distribution or adversarial settings--and, in turn, stronger defenses against adversarial inputs?
- People have been investigating factuality in language models as a countermeasure for the "hallucination" problem. How do we reconcile the goal of factuality with confounding factors, like uncertainty and bias, inherent in human communication?
- Are current and near-future frontier models "only as good as their data"? How much mileage can we get towards grokking new model capabilities or excising undesired capabilities solely by manipulating training data and inputs?
- Weight interpolation (or in the current marketing, "model merging") is black magic that just works. Can we make it work even better?
- Also, starting to think about superalignment!

I'm always open to nuanced discussions about safe, equitable, and rigorously guardrailed deployment of AI systems, as well as the challenges in tech policy and "science of language models" that we must address along the way. I'd also be excited to collaborate on research; if our research interests align, feel free to reach out at branhung (at) alum (dot) mit (dot) edu!