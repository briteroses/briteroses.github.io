<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://briteroses.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://briteroses.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-12-25T23:10:15+00:00</updated><id>https://briteroses.github.io/feed.xml</id><title type="html">blank</title><subtitle>Recent MIT / MIT CSAIL grad, interested in making AI systems more robust and safe </subtitle><entry><title type="html">DPO Mixture Analysis</title><link href="https://briteroses.github.io/blog/2024/dpo-mixture/" rel="alternate" type="text/html" title="DPO Mixture Analysis"/><published>2024-12-25T00:00:00+00:00</published><updated>2024-12-25T00:00:00+00:00</updated><id>https://briteroses.github.io/blog/2024/dpo-mixture</id><content type="html" xml:base="https://briteroses.github.io/blog/2024/dpo-mixture/"><![CDATA[<p>In progress!</p> <p>First, we show that the output distribution has a specific mixture form:</p> <p><strong>Lemma 1.</strong> Assume the preference-tuning objective function used is \eqref{eqn:objective}. With no additional assumptions on \(\mathcal{A}\) and \(\mathcal{U}\), the output distribution of \(\theta\) can be written as:</p> \[p_\theta(y|x)=\alpha_\mathcal{A}(y;x)p_\mathcal{A}(y|x)+\alpha_\mathcal{U}(y;x)p_\mathcal{U}(y|x)\] \[\alpha_\mathcal{A}(y;x)=\frac{1}{Z(x)}\alpha(x)e^{r(y;x)/\beta}, \;\; \alpha_\mathcal{U}(y;x)=\frac{1}{Z(x)}(1-\alpha(x))e^{r(y;x)/\beta}\] <p>where \(Z(x)=\sum_{y \in \supp p_\theta(\cdot | x)} p_{ref}(y|x)e^{r(y;x)/\beta}\) is the partition function of \(p_\theta(y;x)\). \end{lemma}</p> <p>We can then prove a stronger result about how the mixture weights shift:</p> <table> <tbody> <tr> <td><strong>Theorem 2.</strong> Write the “total probability mass” of \(\mathcal{A}\) in $$p_\theta(\cdot</td> <td>x)\(as\)\TPM(\mathcal{A};x) = \sum_{y \in \supp p_\theta(\cdot</td> <td>x)} \alpha_\mathcal{A}(y;x)p_\mathcal{A}(y</td> <td>x)\((and define this respectively for\)\mathcal{U}\(). If the reward function\)r(y;x)$$ satisfies the following additional assumptions:</td> </tr> </tbody> </table> <ul> <li> <table> <tbody> <tr> <td>\(r(y;x)\) and $$p_\mathcal{A}(y</td> <td>x)$$ are positively correlated</td> </tr> </tbody> </table> </li> <li> <table> <tbody> <tr> <td>\(r(y;x)\) and $$p_\mathcal{U}(y</td> <td>x)$$ are negatively correlated</td> </tr> </tbody> </table> </li> <li>\(r\) is finite</li> </ul> <table> <tbody> <tr> <td>then the total probability masses of \(\mathcal{A}\) and \(\mathcal{U}\) in $$p_\theta(\cdot</td> <td>x)$$ satisfy:</td> </tr> </tbody> </table> <p>\(1 &gt; \TPM(\mathcal{A};x) \geq \alpha(x), \;\; 0 &lt; \TPM(\mathcal{U};x) \leq (1-\alpha(x)).\) \end{theorem}</p> <p>The total probability masses of \(\mathcal{A}\) and \(\mathcal{U}\) can be viewed as a measure of the aligned (resp. unaligned) distribution’s influence on the tuned language model’s output distribution. Intuitively, we can interpret Theorem 2 as the following idea: if we select aligned and unaligned distributions that indeed correspond to a level of “alignment” or utility measured by our rewards, then the preference-tuned model draws outputs more heavily from the aligned distribution, while the influence of the unaligned distribution is diminished but not expunged. Crucially, the preference-tuned model still has nonzero probability of producing any harmful or otherwise undesirable output that was possible in the base model.</p> <h2 id="proofs">Proofs</h2> <p>Our proofs of Lemma 1 and Theorem 2 will illuminate further details for how the probability of eliciting a given output from our model is amplified or diminished as a function of the reward.</p> <p>First, the closed-form solution for the preference-tuning objective \(\eqref{eqn:objective}\) is well-known in the KL-constrained RL literature, but we reproduce it here. First, rewrite the objective as:</p> \[\begin{align*} \mathcal{L}(\theta) &amp;= \sum_y p_\theta(y|x)r(y;x) - \beta p_\theta(y|x) \log\left(\frac{p_\theta(y|x)}{p_{ref}(y|x)}\right) \\ &amp;= \beta \sum_y p_\theta(y|x) \Big[ \log e^{r(y;x)/\beta} - \log\left(\frac{p_\theta(y|x)}{p_{ref}(y|x)}\right) \Big] \\ &amp;= -\beta \sum_y p_\theta(y|x) \log\left(\frac{p_\theta(y|x)}{\frac{1}{Z(x)}p_{ref}(y|x)e^{r(y;x)/\beta}}\right) + \beta\log{Z(x)} \end{align*}\] <table> <tbody> <tr> <td>where $$Z(x) = \sum_y p_{ref}(y</td> <td>x)e^{r(y;x)/\beta}\(is the partition function. Notice that\)\frac{1}{Z(x)}p_{ref}(y</td> <td>x)e^{r(y;x)/\beta}\(is a valid probability distribution, so by Gibbs' inequality, the objective\)\mathcal{L}(\theta)$$ is maximized when:</td> </tr> </tbody> </table> \[p_\theta(y|x) = \frac{1}{Z(x)}p_{ref}(y|x)e^{r(y;x)/\beta}.\] <table> <tbody> <tr> <td>Substituting the mixture formulation for $$p_{ref}(y</td> <td>x)$$ and performing a little more algebra gives us:</td> </tr> </tbody> </table> \[p_\theta(y|x)=\alpha_\mathcal{A}(y;x)p_\mathcal{A}(y|x)+\alpha_\mathcal{U}(y;x)p_\mathcal{U}(y|x)\] \[\alpha_\mathcal{A}(y;x)=\frac{1}{Z(x)}\alpha(x)e^{r(y;x)/\beta}, \;\; \alpha_\mathcal{U}(y;x)=\frac{1}{Z(x)}(1-\alpha(x))e^{r(y;x)/\beta}\] <p>as Lemma 1 states.</p> <table> <tbody> <tr> <td>For Theorem 2, we assumed \(r(y;x)\) was positively correlated with $$p_\mathcal{A}(y</td> <td>x)\(and negatively correlated with\)p_\mathcal{U}(y</td> <td>x)\(. Since\)Z(x)\(and\)\alpha(x)\(are constants with respect to\)y\(,\)\alpha_\mathcal{A}(y;x)\(and\)\alpha_\mathcal{U}(y;x)\(are clearly monotonic functions of\)r(y;x)\(, and as a result, the correlation properties are preserved:\)\alpha_\mathcal{A}(y;x)\(is positively correlated with\)p_\mathcal{A}(y</td> <td>x)\(, and\)\alpha_\mathcal{U}(y;x)\(is negatively correlated with\)p_\mathcal{U}(y</td> <td>x)$$.</td> </tr> </tbody> </table> <table> <tbody> <tr> <td>Notice that finiteness of \(r\) implies \(e^{r(y;x)/\beta} &gt; 0\) everywhere, so the output distribution of \(\theta\) must have nonzero probability for any output in \(\supp p_{ref}\). Therefore, $$\supp p_\theta(\cdot</td> <td>x) = \supp p_{ref}(\cdot</td> <td>x)\(. Consider the expectations of\)\alpha_\mathcal{A}(y;x)\(and\)\alpha_\mathcal{U}(y;x)\(over a uniform distribution over (WLOG)\)\supp p_{\theta}(\cdot</td> <td>x)$$, and use the correlation properties:</td> </tr> </tbody> </table> \[\begin{align*} E_{y \in \unif(\supp p_{\theta}(\cdot | x))}[\alpha_\mathcal{A}(y;x)p_\mathcal{A}(y|x)] &amp;&gt; E_y[\alpha_\mathcal{A}(y;x)]E_y[p_\mathcal{A}(y|x)] \\ &amp;= \frac{1}{Z(x)}\alpha(x)E_y[e^{r(y;x)/\beta}]E_y[p_\mathcal{A}(y|x)]. \end{align*}\] \[\begin{align*} E_y[\alpha_\mathcal{U}(y;x)p_\mathcal{U}(y|x)] &amp;&lt; E_y[\alpha_\mathcal{U}(y;x)]E_y[p_\mathcal{U}(y|x)] \\ &amp;= \frac{1}{Z(x)}(1-\alpha(x))E_y[e^{r(y;x)/\beta}]E_y[p_\mathcal{U}(y|x)]. \end{align*}\] <table> <tbody> <tr> <td>(For the sake of brevity, we mostly omit writing the distribution over which the expectation is taken; for the rest of the proof, \(E_y[\cdot]\) is shorthand for $$E_{y \in \unif(\supp p_{\theta}(\cdot</td> <td>x))}[\cdot]\(.) The\)E_y[e^{r(y;x)/\beta}]$$ and partition function terms vanish upon dividing the inequalities:</td> </tr> </tbody> </table> \[\frac{E_y[\alpha_\mathcal{A}(y;x)p_\mathcal{A}(y|x)]}{E_y[\alpha_\mathcal{U}(y;x)p_\mathcal{U}(y|x)]} &gt; \frac{E_y[\alpha(x)p_\mathcal{A}(y|x)]}{E_y[(1-\alpha(x))p_\mathcal{U}(y|x)]}\] <p>Reciprocating the inequality, adding 1 to both sides, and reciprocating again yields:</p> \[\frac{E_y[\alpha_\mathcal{A}(y;x)p_\mathcal{A}(y|x)]}{E_y[p_\theta(y|x)]} &gt; \frac{E_y[\alpha(x)p_\mathcal{A}(y|x)]}{E_y[p_{ref}(y|x)]}\] <table> <tbody> <tr> <td>Finally, we multiply all expectations by $$\card(\supp p_\theta(\cdot</td> <td>x))$$. Using the property that</td> </tr> </tbody> </table> \[\card(\supp p) \cdot E_{y \in \unif(\supp p)}[p(x)] = 1\] <table> <tbody> <tr> <td>for any discrete probability distribution \(p\), along with our earlier observation that $$\supp p_\theta(\cdot</td> <td>x) = \supp p_{ref}(\cdot</td> <td>x)\(, we see that the expectations of\)p_\theta(y</td> <td>x)\(and\)p_{ref}(y</td> <td>x)\(vanish. On the other hand, multiplying the expectation of\)\alpha_\mathcal{A}(y;x)p_\mathcal{A}(y</td> <td>x)\(by\)\card(\supp p_\theta(\cdot</td> <td>x))$$ gives the total probability mass. We recover:</td> </tr> </tbody> </table> \[\TPM(\mathcal{A};x) &gt; \alpha(x) \;\;\;\;\;\;\;\; \text{and likewise, } \TPM(\mathcal{U};x) &lt; (1-\alpha(x)).\]]]></content><author><name></name></author><category term="research"/><category term="math"/><category term="dpo"/><category term="machine-learning"/><summary type="html"><![CDATA[Mathematical analysis of DPO mixture distributions]]></summary></entry></feed>