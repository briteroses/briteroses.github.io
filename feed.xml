<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://briteroses.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://briteroses.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-12-26T00:55:10+00:00</updated><id>https://briteroses.github.io/feed.xml</id><title type="html">blank</title><subtitle>Recent MIT / MIT CSAIL grad, interested in making AI systems more robust and safe </subtitle><entry><title type="html">DPO Mixture Analysis</title><link href="https://briteroses.github.io/blog/2024/dpo-mixture/" rel="alternate" type="text/html" title="DPO Mixture Analysis"/><published>2024-12-25T00:00:00+00:00</published><updated>2024-12-25T00:00:00+00:00</updated><id>https://briteroses.github.io/blog/2024/dpo-mixture</id><content type="html" xml:base="https://briteroses.github.io/blog/2024/dpo-mixture/"><![CDATA[<p>In progress!</p> <p>First, we show that the output distribution has a specific mixture form:</p> <p><strong>Lemma 1.</strong> Assume the preference-tuning objective function used is [TODO]. With no additional assumptions on \(\mathcal{A}\) and \(\mathcal{U}\), the output distribution of \(\theta\) can be written as:</p> \[p_\theta(y|x)=\alpha_\mathcal{A}(y;x)p_\mathcal{A}(y|x)+\alpha_\mathcal{U}(y;x)p_\mathcal{U}(y|x)\] \[\alpha_\mathcal{A}(y;x)=\frac{1}{Z(x)}\alpha(x)e^{r(y;x)/\beta}, \;\; \alpha_\mathcal{U}(y;x)=\frac{1}{Z(x)}(1-\alpha(x))e^{r(y;x)/\beta}\] <p>where <code class="language-plaintext highlighter-rouge">$$Z(x)=\sum_{y \in \supp p_\theta(\cdot | x)} p_{ref}(y|x)e^{r(y;x)/\beta}$$</code> is the partition function of <code class="language-plaintext highlighter-rouge">$$p_\theta(y;x)$$</code>.</p> <p>We can then prove a stronger result about how the mixture weights shift:</p> <p><strong>Theorem 2.</strong> Write the “total probability mass” of <code class="language-plaintext highlighter-rouge">$$\mathcal{A}$$</code> in <code class="language-plaintext highlighter-rouge">$$p_\theta(\cdot | x)$$</code> as <code class="language-plaintext highlighter-rouge">$$\TPM(\mathcal{A};x) = \sum_{y \in \supp p_\theta(\cdot | x)} \alpha_\mathcal{A}(y;x)p_\mathcal{A}(y|x)$$</code> (and define this respectively for <code class="language-plaintext highlighter-rouge">$$\mathcal{U}$$</code>). If the reward function <code class="language-plaintext highlighter-rouge">$$r(y;x)$$</code> satisfies the following additional assumptions:</p> <ul> <li><code class="language-plaintext highlighter-rouge">$$r(y;x)$$</code> and <code class="language-plaintext highlighter-rouge">$$p_\mathcal{A}(y|x)$$</code> are positively correlated</li> <li><code class="language-plaintext highlighter-rouge">$$r(y;x)$$</code> and <code class="language-plaintext highlighter-rouge">$$p_\mathcal{U}(y|x)$$</code> are negatively correlated</li> <li><code class="language-plaintext highlighter-rouge">$$r$$</code> is finite</li> </ul> <p>then the total probability masses of <code class="language-plaintext highlighter-rouge">$$\mathcal{A}$$</code> and <code class="language-plaintext highlighter-rouge">$$\mathcal{U}$$</code> in <code class="language-plaintext highlighter-rouge">$$p_\theta(\cdot | x)$$</code> satisfy:</p> \[1 &gt; \TPM(\mathcal{A};x) \geq \alpha(x), \;\; 0 &lt; \TPM(\mathcal{U};x) \leq (1-\alpha(x)).\] <p>The total probability masses of <code class="language-plaintext highlighter-rouge">$$\mathcal{A}$$</code> and <code class="language-plaintext highlighter-rouge">$$\mathcal{U}$$</code> can be viewed as a measure of the aligned (resp. unaligned) distribution’s influence on the tuned language model’s output distribution. Intuitively, we can interpret Theorem 2 as the following idea: if we select aligned and unaligned distributions that indeed correspond to a level of “alignment” or utility measured by our rewards, then the preference-tuned model draws outputs more heavily from the aligned distribution, while the influence of the unaligned distribution is diminished but not expunged. Crucially, the preference-tuned model still has nonzero probability of producing any harmful or otherwise undesirable output that was possible in the base model.</p> <h2 id="proofs">Proofs</h2> <p>Our proofs of Lemma 1 and Theorem 2 will illuminate further details for how the probability of eliciting a given output from our model is amplified or diminished as a function of the reward.</p> <p>First, the closed-form solution for the KL-constrained RL objective is well-known, but we reproduce it here. First, rewrite the objective as:</p> \[\begin{align*} \mathcal{L}(\theta) &amp;= \sum_y p_\theta(y|x)r(y;x) - \beta p_\theta(y|x) \log\left(\frac{p_\theta(y|x)}{p_{ref}(y|x)}\right) \\ &amp;= \beta \sum_y p_\theta(y|x) \Big[ \log e^{r(y;x)/\beta} - \log\left(\frac{p_\theta(y|x)}{p_{ref}(y|x)}\right) \Big] \\ &amp;= -\beta \sum_y p_\theta(y|x) \log\left(\frac{p_\theta(y|x)}{\frac{1}{Z(x)}p_{ref}(y|x)e^{r(y;x)/\beta}}\right) + \beta\log{Z(x)} \end{align*}\] <p>where <code class="language-plaintext highlighter-rouge">$$Z(x) = \sum_y p_{ref}(y|x)e^{r(y;x)/\beta}$$</code> is the partition function. Notice that <code class="language-plaintext highlighter-rouge">$$\frac{1}{Z(x)}p_{ref}(y|x)e^{r(y;x)/\beta}$$</code> is a valid probability distribution, so by Gibbs’ inequality, the objective <code class="language-plaintext highlighter-rouge">$$\mathcal{L}(\theta)$$</code> is maximized when:</p> \[p_\theta(y|x) = \frac{1}{Z(x)}p_{ref}(y|x)e^{r(y;x)/\beta}.\] <p>Substituting the mixture formulation for <code class="language-plaintext highlighter-rouge">$$p_{ref}(y|x)$$</code> and performing a little more algebra gives us:</p> \[p_\theta(y|x)=\alpha_\mathcal{A}(y;x)p_\mathcal{A}(y|x)+\alpha_\mathcal{U}(y;x)p_\mathcal{U}(y|x)\] \[\alpha_\mathcal{A}(y;x)=\frac{1}{Z(x)}\alpha(x)e^{r(y;x)/\beta}, \;\; \alpha_\mathcal{U}(y;x)=\frac{1}{Z(x)}(1-\alpha(x))e^{r(y;x)/\beta}\] <p>as Lemma 1 states.</p> <p>For Theorem 2, we assumed <code class="language-plaintext highlighter-rouge">$$r(y;x)$$</code> was positively correlated with <code class="language-plaintext highlighter-rouge">$$p_\mathcal{A}(y|x)$$</code> and negatively correlated with <code class="language-plaintext highlighter-rouge">$$p_\mathcal{U}(y|x)$$</code>. Since <code class="language-plaintext highlighter-rouge">$$Z(x)$$</code> and <code class="language-plaintext highlighter-rouge">$$\alpha(x)$$</code> are constants with respect to <code class="language-plaintext highlighter-rouge">$$y$$</code>, <code class="language-plaintext highlighter-rouge">$$\alpha_\mathcal{A}(y;x)$$</code> and <code class="language-plaintext highlighter-rouge">$$\alpha_\mathcal{U}(y;x)$$</code> are clearly monotonic functions of <code class="language-plaintext highlighter-rouge">$$r(y;x)$$</code>, and as a result, the correlation properties are preserved: <code class="language-plaintext highlighter-rouge">$$\alpha_\mathcal{A}(y;x)$$</code> is positively correlated with <code class="language-plaintext highlighter-rouge">$$p_\mathcal{A}(y|x)$$</code>, and <code class="language-plaintext highlighter-rouge">$$\alpha_\mathcal{U}(y;x)$$</code> is negatively correlated with <code class="language-plaintext highlighter-rouge">$$p_\mathcal{U}(y|x)$$</code>.</p> <p>Notice that finiteness of <code class="language-plaintext highlighter-rouge">$$r$$</code> implies <code class="language-plaintext highlighter-rouge">$$e^{r(y;x)/\beta} &gt; 0$$</code> everywhere, so the output distribution of <code class="language-plaintext highlighter-rouge">$$\theta$$</code> must have nonzero probability for any output in <code class="language-plaintext highlighter-rouge">$$\supp p_{ref}$$</code>. Therefore, <code class="language-plaintext highlighter-rouge">$$\supp p_\theta(\cdot | x) = \supp p_{ref}(\cdot | x)$$</code>. Consider the expectations of <code class="language-plaintext highlighter-rouge">$$\alpha_\mathcal{A}(y;x)$$</code> and <code class="language-plaintext highlighter-rouge">$$\alpha_\mathcal{U}(y;x)$$</code> over a uniform distribution over (WLOG) <code class="language-plaintext highlighter-rouge">$$\supp p_{\theta}(\cdot | x)$$</code>, and use the correlation properties:</p> \[\begin{align*} E_{y \in \unif(\supp p_{\theta}(\cdot | x))}[\alpha_\mathcal{A}(y;x)p_\mathcal{A}(y|x)] &amp;&gt; E_y[\alpha_\mathcal{A}(y;x)]E_y[p_\mathcal{A}(y|x)] \\ &amp;= \frac{1}{Z(x)}\alpha(x)E_y[e^{r(y;x)/\beta}]E_y[p_\mathcal{A}(y|x)]. \end{align*}\] \[\begin{align*} E_y[\alpha_\mathcal{U}(y;x)p_\mathcal{U}(y|x)] &amp;&lt; E_y[\alpha_\mathcal{U}(y;x)]E_y[p_\mathcal{U}(y|x)] \\ &amp;= \frac{1}{Z(x)}(1-\alpha(x))E_y[e^{r(y;x)/\beta}]E_y[p_\mathcal{U}(y|x)]. \end{align*}\] <p>(For the sake of brevity, we mostly omit writing the distribution over which the expectation is taken; for the rest of the proof, <code class="language-plaintext highlighter-rouge">$$E_y[\cdot]$$</code> is shorthand for <code class="language-plaintext highlighter-rouge">$$E_{y \in \unif(\supp p_{\theta}(\cdot | x))}[\cdot]$$</code>.) The <code class="language-plaintext highlighter-rouge">$$E_y[e^{r(y;x)/\beta}]$$</code> and partition function terms vanish upon dividing the inequalities:</p> \[\frac{E_y[\alpha_\mathcal{A}(y;x)p_\mathcal{A}(y|x)]}{E_y[\alpha_\mathcal{U}(y;x)p_\mathcal{U}(y|x)]} &gt; \frac{E_y[\alpha(x)p_\mathcal{A}(y|x)]}{E_y[(1-\alpha(x))p_\mathcal{U}(y|x)]}\] <p>Reciprocating the inequality, adding 1 to both sides, and reciprocating again yields:</p> \[\frac{E_y[\alpha_\mathcal{A}(y;x)p_\mathcal{A}(y|x)]}{E_y[p_\theta(y|x)]} &gt; \frac{E_y[\alpha(x)p_\mathcal{A}(y|x)]}{E_y[p_{ref}(y|x)]}\] <p>Finally, we multiply all expectations by <code class="language-plaintext highlighter-rouge">$$\card(\supp p_\theta(\cdot | x))$$</code>. Using the property that</p> \[\card(\supp p) \cdot E_{y \in \unif(\supp p)}[p(x)] = 1\] <p>for any discrete probability distribution <code class="language-plaintext highlighter-rouge">$$p$$</code>, along with our earlier observation that <code class="language-plaintext highlighter-rouge">$$\supp p_\theta(\cdot | x) = \supp p_{ref}(\cdot | x)$$</code>, we see that the expectations of <code class="language-plaintext highlighter-rouge">$$p_\theta(y|x)$$</code> and <code class="language-plaintext highlighter-rouge">$$p_{ref}(y|x)$$</code> vanish. On the other hand, multiplying the expectation of <code class="language-plaintext highlighter-rouge">$$\alpha_\mathcal{A}(y;x)p_\mathcal{A}(y|x)$$</code> by <code class="language-plaintext highlighter-rouge">$$\card(\supp p_\theta(\cdot | x))$$</code> gives the total probability mass. We recover:</p> \[\TPM(\mathcal{A};x) &gt; \alpha(x) \;\;\;\;\;\;\;\; \text{and likewise, } \TPM(\mathcal{U};x) &lt; (1-\alpha(x)).\]]]></content><author><name></name></author><category term="research"/><category term="math"/><category term="dpo"/><category term="machine-learning"/><summary type="html"><![CDATA[Mathematical analysis of DPO mixture distributions]]></summary></entry></feed>